apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: data-quality
  namespace: spark
spec:
  arguments:
  - --integrity-config
  - |
    sink:
      schema: dvdrental
      catalog: product
      catalogProps:
        # catalog-impl: hadoop
        # warehouse: /tmp/warehouse
        catalog-impl: org.apache.iceberg.hive.HiveCatalog
        warehouse: s3a://my-datalake/alluvial/.warehouse/
        uri: thrift://hive-local-metastore.hive.svc:9083
        io-impl: org.apache.iceberg.aws.s3.S3FileIO
        # see: org.apache.iceberg.aws.AwsProperties
        s3.endpoint: http://minio.minio.svc:9000
        s3.access-key-id: admin
        s3.secret-access-key: SuperSecr3t
        s3.path-style-access: true

    source:
      jdbcUrl: jdbc:mysql://mysql.mysql.svc:3306/sakila
      driver: com.mysql.cj.jdbc.Driver
      type: mysql
      database: sakila
      hostname: mysql.mysql.svc
      port: 3306
      username: root
      password: SuperSecr3t

    checkers:
      - name: schema_check
        checkerClass: dev.alluvial.data.integrity.checkers.CheckMysqlSchema
      - name: duplication_check
        checkerClass: dev.alluvial.data.integrity.checkers.Duplication
        params:
          uniqueColumns: ['language_id']

    integrity:
      - group: group1
        tables: ['.*']
        checkers:
          - schema_check
      - group: group5
        tables: ['film.*', 'location']
        checkers:
          - duplication_check
  type: Java
  mode: cluster
  imagePullPolicy: IfNotPresent
  image: "hub.teko.vn/data/alluvial-data-quality:0.1.9-dev-3"
  mainClass: dev.alluvial.data.integrity.MainKt
  mainApplicationFile: "local:///app/alluvial-data-quality.jar"
  sparkVersion: "3.3"
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    serviceAccount: spark
    env:
    - name: HADOOP_OPTIONAL_TOOLS
      value: hadoop-aws,hadoop-iceberg-aws
    - name: AWS_REGION
      value: aws-global
    - name: JAVA_VERSION
      value: "11"
  executor:
    cores: 1
    instances: 1
    memory: "512m"
    env:
    - name: HADOOP_OPTIONAL_TOOLS
      value: hadoop-aws,hadoop-iceberg-aws
    - name: AWS_REGION
      value: aws-global
    - name: JAVA_VERSION
      value: "11"
  hadoopConf:
    fs.s3a.bucket.my-datalake.access.key: admin
    fs.s3a.bucket.my-datalake.access.secret: SuperSecr3t
    fs.s3a.bucket.my-datalake.endpoint: http://minio.minio.svc:9000
    fs.s3a.bucket.my-datalake.path.style.access: "true"
    fs.s3a.bucket.my-datalake.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
  sparkConf:
    spark.sql.iceberg.handle-timestamp-without-timezone: "true"
    spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
    spark.sql.catalog.product: org.apache.iceberg.spark.SparkCatalog
    spark.sql.catalog.product.type: hive
    spark.sql.catalog.product.uri: thrift://hive-local-metastore.hive.svc:9083
    spark.sql.catalog.product.warehouse: s3a://my-datalake/alluvial/.warehouse/
    spark.sql.catalog.product.io-impl: org.apache.iceberg.aws.s3.S3FileIO
    spark.sql.catalog.product.s3.endpoint: http://minio.minio.svc:9000
    spark.sql.catalog.product.s3.access-key-id: admin
    spark.sql.catalog.product.s3.secret-access-key: SuperSecr3t
    spark.sql.catalog.product.s3.path-style-access: "true"
